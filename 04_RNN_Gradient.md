# 📘 NLP 이론 4강 정리



------------------------------------------------------------------------

## 1. 긴 문장 입력의 문제점

-   RNN은 입력 문장을 순차적으로 처리하며 Hidden state에 정보를 저장
-   문제: 정답 단어와 관련된 단어가 너무 멀리 떨어져 있으면 학습이
    어려움
    -   예: "안녕, \_\_\_\_야?" → 답: "철수"
    -   긴 시퀀스에서 중요한 정보가 사라질 수 있음

------------------------------------------------------------------------

## 2. RNN의 문제점

1.  입력된 단어 정보가 Hidden state에 저장됨
2.  여러 timestep을 거치며 Hidden state를 갱신
3.  마지막 timestep의 Hidden state로 예측 수행

**문제점**\
- 모든 정보를 고정된 크기의 Hidden state에 저장해야 함
- Forward propagation 중 동일 가중치를 계속 곱하게 됨 → Gradient 문제가
발생

------------------------------------------------------------------------

## 3. Forward & Backpropagation

-   **Forward propagation**
    -   `y = d·h_t + e (Bias term)`
    -   Bias term은 예측을 보정하는 파라미터 역할
-   **Backpropagation**
    -   합성 함수 미분 구조 → Gradient가 반복적으로 곱해짐
    -   등비수열 성격 → 공비 크기에 따라 Gradient 폭주 or 소실

------------------------------------------------------------------------

## 4. Exploding / Vanishing Gradient

-   수식적으로 Hidden state는 가중치 행렬을 반복 곱하는 과정과 유사\
-   가중치 행렬을 Eigen decomposition 하면, Eigenvalue 크기에 따라
    결과가 달라짐

### 4.1 Vanishing Gradient (소실 기울기)

-   **정의**: 역전파 과정에서 기울기가 점점 작아져 초기 층까지 전달되지
    않는 현상\
-   **원인**:
    -   동일 가중치 반복 곱셈
    -   Sigmoid/tanh 같은 비선형 함수에서 Gradient가 급격히 줄어듦
    -   Eigenvalue 절댓값 \< 1
-   **결과**: 장기 의존성(Long-term dependency) 학습 실패 → 중요한 과거
    정보 소실

### 4.2 Exploding Gradient (폭주 기울기)

-   **정의**: 역전파 과정에서 Gradient가 기하급수적으로 커지는 현상
-   **원인**:
    -   가중치 \> 1 → Gradient 값이 계속 커짐
    -   Eigenvalue 절댓값 \> 1
-   **결과**: 가중치 업데이트가 비정상적으로 커져 학습 불안정, 발산

------------------------------------------------------------------------

## 5. 해결 방안

-   **LSTM(Long Short-Term Memory)**
    -   Gradient 소실 문제를 완화
    -   게이트 구조를 통해 정보 선택적 전달/저장

------------------------------------------------------------------------

## 6. Summary

-   긴 시퀀스 학습 시 RNN은 **Exploding/Vanishing Gradient** 문제에
    취약
-   원인: Forward/Backward 과정에서 동일 가중치 반복 곱셈
-   Eigenvalue 크기에 따라 Gradient 폭주(\>1) 또는 소실(\<1) 발생
-   해결책: **LSTM** 같은 개선된 구조 활용
