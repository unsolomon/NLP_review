# Transformer 핵심 정리

## 1. Transformer 개요

-   제안 논문: *Attention Is All You Need* (Vaswani et al., 2017)
-   기존 RNN, CNN 없이 **Self-Attention** 메커니즘만으로 시퀀스를 처리
-   Encoder-Decoder 구조를 유지하며, 긴 문맥 처리와 병렬 연산에서 강력한
    성능을 보임

------------------------------------------------------------------------

## 2. Self-Attention

-   입력 시퀀스의 각 단어가 **다른 모든 단어와 관계**를 계산해 새로운
    표현을 생성
-   핵심 아이디어: **Query(Q), Key(K), Value(V)**
    -   Query(Q): 현재 단어가 "무엇을 찾고 싶은지"
    -   Key(K): 단어의 "특성"
    -   Value(V): 단어의 "실제 정보"

### 계산 과정

1.  입력 단어 임베딩 → Q, K, V 생성 (가중치 행렬 곱)
2.  어텐션 점수 = Q · Kᵀ (내적, 유사도 계산)
3.  점수가 커지면 Softmax 분포가 한쪽으로 치우치므로 **스케일링** 적용:
    \[ `\frac{QK^T}{\sqrt{d_k}}`{=tex} \]
4.  Softmax 결과 × V (가중합) → 최종 출력

------------------------------------------------------------------------

## 3. Self-Attention의 변형

| 방법                   | 설명                             | 특징                   |
|------------------------|----------------------------------|------------------------|
| **Dot-Product Attention** | Q·K 내적 기반                      | 간단하고 빠름            |
| **Scaled Dot-Product Attention** | 내적값을 √dₖ로 나눠 안정성 확보        | 긴 문맥에서도 안정적       |
| **Query Batch**        | 여러 쿼리를 행렬 형태로 동시에 처리     | 병렬 연산에 최적           |
| **Multi-Head Attention** | 여러 Attention Head를 병렬 수행 후 결합 | 다양한 시각에서 정보 추출   |


------------------------------------------------------------------------

## 4. Multi-Head Attention

-   하나의 Attention은 단일 시각에서만 정보를 본다 → **제한적**
-   여러 개의 Head를 두어 서로 다른 관점에서 Attention 수행
-   각 Head 출력 → Concatenate → 최종 선형 변환
-   장점:
    -   다양한 의미적 관계 학습
    -   모델 표현력 강화

------------------------------------------------------------------------

## 5. Self-Attention vs RNN/CNN

-   **RNN**: 시퀀스 순차적 처리 → 학습 속도 느림, Long-term dependency
    문제
-   **CNN**: 병렬 연산 가능, 하지만 fixed window 크기 제약
-   **Self-Attention**:
    -   모든 단어와 직접 연결 (경로 길이 O(1))
    -   병렬 처리 가능
    -   해석 가능 (어떤 단어가 집중되었는지 시각화 가능)

------------------------------------------------------------------------

## 6. 핵심 정리

-   RNN의 한계를 해결 → Self-Attention
-   Dot-product + Scaling → 안정된 학습
-   Multi-head → 표현력 극대화
-   Transformer는 이후 BERT, GPT 등 모든 현대 NLP 모델의 기반이 됨
