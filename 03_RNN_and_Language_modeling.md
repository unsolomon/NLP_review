

# RNN(Recurrent Neural Network) \& Language Modeling 핵심 정리

## RNN 개념 요약

- RNN은 가변 길이의 시퀀스 데이터를 입력으로 받아 처리하는 신경망 모델로, 각 타임스텝(time step)마다 반복적인 수식이 적용된다.
- 입력 텍스트는 정수로 인코딩되고, 임베딩 레이어를 거쳐 RNN에 입력됨.
- RNN은 이전 타임스텝의 은닉 상태 \$ h_{t-1} \$과 현재 입력 \$ x_t \$를 받아 은닉 상태 \$ h_t \$ 계산.

$$
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t)
$$
- 출력은 은닉 상태 \$ h_t \$를 통해 예측하며, Softmax 연산으로 다음 단어 확률 분포 계산.

$$
y_t = W_{hy} h_t
$$


## RNN 구조

- **Rolled RNN**: 모든 타임스텝의 연산을 하나의 재귀 함수 형태로 표현.

$$
h_t = f_W(h_{t-1}, x_t)
$$
    - 장점: 메모리 효율적, 계산 속도 빠름
    - 단점: 시각화 및 디버깅 어려움
- **Unrolled RNN**: 각 타임스텝을 펼쳐서 시각화한 구조.
    - 각 시점별 상태 변화를 직관적으로 이해 가능
    - 단점: 메모리와 계산 비용이 큼


## RNN의 주요 변형

- **Multi-layer RNN**: 여러 RNN 층을 쌓아 더 복잡한 패턴 학습
- **Bidirectional RNN**: 과거 뿐 아니라 미래 정보도 동시에 활용하여 시퀀스를 양방향 처리


## RNN 사용 형태

- **one-to-one**: 일반 신경망 (예: CNN)
- **one-to-many**: 이미지 캡션 생성 (한 이미지 → 여러 단어)
- **many-to-one**: 문장 감정 분류 (여러 단어 → 한 감정)
- **many-to-many**: 기계 번역, 비디오 프레임 분석 등


## Language Modeling

- 목표: 이전 단어 시퀀스를 바탕으로 다음 단어를 예측
- 입력은 문자 단위/단어 단위로 정수 인코딩 후 One-hot 또는 임베딩 처리
- 확률 예측을 위해 Softmax 적용 및 크로스 엔트로피 손실 함수 사용
- 전체 시퀀스에 대해 누적된 손실로 모델 학습 진행


## Backpropagation Through Time (BPTT)

- RNN 학습 시 시퀀스 전체를 펼쳐서 역전파 진행
- 가중치 \$ W_{xh}, W_{hy}, W_{hh} \$ 는 모든 타임스텝에서 공유
- 문제점: 긴 시퀀스일 경우 계산 비용과 메모리 요구가 급증함


## Truncated BPTT

- 긴 시퀀스를 짧은 청크(chunk)로 나누어 역전파 진행
- 은닉 상태는 청크 간에 전달 유지
- 계산량 및 메모리 사용 감소


